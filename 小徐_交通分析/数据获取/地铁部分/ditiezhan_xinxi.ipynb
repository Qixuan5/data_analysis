{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85acbaf-3831-48f4-b0f3-35a004018846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取城市北京的数据\n",
      "开始爬取城市上海的数据\n",
      "开始爬取城市广州的数据\n",
      "开始爬取城市深圳的数据\n",
      "开始爬取城市武汉的数据\n",
      "开始爬取城市天津的数据\n",
      "开始爬取城市南京的数据\n",
      "开始爬取城市香港的数据\n",
      "开始爬取城市重庆的数据\n",
      "开始爬取城市杭州的数据\n",
      "开始爬取城市沈阳的数据\n",
      "开始爬取城市大连的数据\n",
      "开始爬取城市成都的数据\n",
      "开始爬取城市长春的数据\n",
      "开始爬取城市苏州的数据\n",
      "开始爬取城市佛山的数据\n",
      "开始爬取城市昆明的数据\n",
      "开始爬取城市西安的数据\n",
      "开始爬取城市郑州的数据\n",
      "开始爬取城市长沙的数据\n",
      "开始爬取城市宁波的数据\n",
      "开始爬取城市无锡的数据\n",
      "开始爬取城市青岛的数据\n",
      "开始爬取城市南昌的数据\n",
      "开始爬取城市福州的数据\n",
      "开始爬取城市东莞的数据\n",
      "开始爬取城市南宁的数据\n",
      "开始爬取城市合肥的数据\n",
      "开始爬取城市贵阳的数据\n",
      "开始爬取城市厦门的数据\n",
      "开始爬取城市哈尔滨的数据\n",
      "开始爬取城市石家庄的数据\n",
      "开始爬取城市乌鲁木齐的数据\n",
      "开始爬取城市温州的数据\n",
      "开始爬取城市济南的数据\n",
      "开始爬取城市兰州的数据\n",
      "开始爬取城市常州的数据\n",
      "开始爬取城市徐州的数据\n",
      "开始爬取城市太原的数据\n",
      "开始爬取城市呼和浩特的数据\n",
      "开始爬取城市洛阳的数据\n"
     ]
    }
   ],
   "source": [
    "# 首先引入所需要的包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 城市信息列表\n",
    "city_info_list = []\n",
    "# 发送 GET 请求获取网页内容\n",
    "url = 'http://map.amap.com/subway/index.html'\n",
    "response = requests.get(url)\n",
    "# 第一步：爬取两个 div 中的城市数据（包括 ID 和拼音），生成城市集合\n",
    "if response.status_code == 200:\n",
    "    # 解码\n",
    "    response_content = response.content.decode('utf-8')\n",
    "    # 使用 Beautiful Soup 解析网页内容\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    # 查找标题\n",
    "    title = soup.title\n",
    "    # 通过Beautiful Soup来找到城市信息元素，并提取这个元素的信息\n",
    "    for soup_a in soup.find('div', class_='city-list fl').find_all('a'):\n",
    "        city_name_py = soup_a['cityname']\n",
    "        city_id = soup_a['id']\n",
    "        city_name_ch = soup_a.get_text()\n",
    "        city_info_list.append({'name_py': city_name_py, 'id': city_id, 'name_ch': city_name_ch})\n",
    "\t# 获取未显示出来的城市列表\n",
    "    for soup_a in soup.find('div', class_='more-city-list').find_all('a'):\n",
    "        city_name_py = soup_a['cityname']\n",
    "        city_id = soup_a['id']\n",
    "        city_name_ch = soup_a.get_text()\n",
    "        city_info_list.append({'name_py': city_name_py, 'id': city_id, 'name_ch': city_name_ch})\n",
    "else:\n",
    "    print(\"无法获取网页内容\")\n",
    "\n",
    "for city_info in city_info_list:\n",
    "    city_id = city_info.get(\"id\")\n",
    "    city_name = city_info.get(\"name_py\")\n",
    "    city_name_ch = city_info.get(\"name_ch\")\n",
    "    print(\"开始爬取城市\" + city_name_ch + \"的数据\")\n",
    "    city_lines_list = []\n",
    "    # 第二步：遍历城市集合，构造每一个城市的 url,并下载数据\n",
    "    # 构造每个城市的url\n",
    "    url = \"http://map.amap.com/service/subway?_1818387860087&srhdata=\" + city_id + '_drw_' + city_name + '.json'\n",
    "    res = requests.get(url)\n",
    "    content = res.content.decode('utf-8')\n",
    "    # 将内容字符串转换成json对象\n",
    "    content_json = json.loads(content)\n",
    "    # 提取该城市的所有地铁线list\n",
    "    line_info_list = content_json.get(\"l\")\n",
    "    # 第三步：开始处理每一个地铁线，提取内容到dataframe中\n",
    "    for line_info in line_info_list:\n",
    "        # 地铁线名字\n",
    "        line_name = line_info[\"kn\"]\n",
    "        # 处理地铁线站点\n",
    "        df_per_zd = pd.DataFrame(line_info[\"st\"])\n",
    "        df_per_zd = df_per_zd[['n', 'sl', 'poiid', 'sp']]\n",
    "        df_per_zd['gd经度'] = df_per_zd['sl'].apply(lambda x: x.split(',')[0])\n",
    "        df_per_zd['gd纬度'] = df_per_zd['sl'].apply(lambda x: x.split(',')[1])\n",
    "        df_per_zd.drop('sl', axis=1, inplace=True)\n",
    "        df_per_zd['路线名称'] = line_info['ln']\n",
    "        df_per_zd['城市名称'] = city_name_ch\n",
    "        df_per_zd.rename(columns={\"n\": \"站点名称\", \"poiid\" : \"POI编号\", \"sp\" : \"拼音名称\"}, inplace=True)\n",
    "        # 先将这条地铁线处理过的dataframe存起来，后面再储存到一张表里\n",
    "        city_lines_list.append(df_per_zd)\n",
    "    # 这段代码就是将地铁线数据列表聚合到一张表里，形成每个城市的地铁站数据\n",
    "    city_subway_data = pd.concat(city_lines_list, ignore_index=True)\n",
    "    # 第四步：将处理好的文件保存为xlsx\n",
    "    city_subway_data.to_excel(city_name_ch + '.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e9448-6d54-49ff-b83c-f672f63fe983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a1a80-2b38-4f1c-aaea-97bf4c7d038e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
